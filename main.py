# -*- coding: utf-8 -*-
"""NLP_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DmWCY17AfwNP1qNRWnNOHeuGz7p8PRhG

## Installing and import packages
"""

# !pip install googletrans==3.1.0a0

!pip install transformers datasets eli5

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

import nltk
nltk.download('punkt')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

import eli5

import re, string
from collections import defaultdict, Counter
from itertools import chain

import torch
from torch.utils.data import Dataset, DataLoader

import pyarrow as pa
from datasets import Dataset

import transformers as ppb # pytorch transformers
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification, 
    get_linear_schedule_with_warmup,
    DataCollatorWithPadding, 
    pipeline,
    Trainer,
    TrainingArguments)

from tqdm.notebook import tqdm

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix
import seaborn as sn

from google.colab import drive
from google.colab import output

# from googletrans import Translator
# translator = Translator()

drive.mount('/content/drive')

"""## Dataset of english sarcasm -- translation"""

df_en = pd.read_csv('/content/drive/MyDrive/Sarcastic_сomments/train.csv')

df_en.head()

df_ru = pd.DataFrame(columns=['comment', 'label'])

for index, row in df_en.iterrows():
    df_ru.loc[index + 1] = [translator.translate(row['text'], dest='ru').text] + [row['Y']]
    if not(index % 100):
      print(f"{index} is translated")

df_ru.head()

"""Все переведено и сохранено (НЕ НАЖИМАТЬ!! пересчитывать достаточно долго)"""

df_ru.to_csv('/content/drive/MyDrive/Sarcastic_сomments/train_ru.csv', index=False)

"""Для последующих запусков (считываем датафрейм)"""

df_ru = pd.read_csv('/content/drive/MyDrive/Sarcastic_сomments/train_ru.csv')

"""## Dataset of Russian jokes -- read_csv"""

df_jokes = pd.read_csv('/content/drive/MyDrive/Sarcastic_сomments/jokes.csv')
df_jokes = df_jokes.drop(columns=['theme', 'rating'])
df_jokes = df_jokes.rename(columns={"text": "comment"})
df_jokes['label'] = 1

df_jokes.head()

df_jokes.to_csv('/content/drive/MyDrive/Sarcastic_сomments/train_ru_jokes.csv', index=False)

"""## Make the final dataset"""

df_ru_jokes = pd.read_csv('/content/drive/MyDrive/Sarcastic_сomments/train_ru_jokes.csv')
df_en_sarcasm_or_not = pd.read_csv('/content/drive/MyDrive/Sarcastic_сomments/train_ru.csv')

df_en_non_sarcasm = df_en_sarcasm_or_not.query('label==0')
df_en_sarcasm = df_en_sarcasm_or_not.query('label==1')

print("Translated non-sarcasm: ", len(df_en_non_sarcasm))
print("Translated sarcasm: ", len(df_en_sarcasm))
print("Russian jokes: ", len(df_ru_jokes))

# # big dataset without jokes
# non_sarcasm_num = 9500
# sarcasm_num = 9500
# jokes_num = 0

# big dataset with jokes
non_sarcasm_num = 10000
sarcasm_num = 5000
jokes_num = 5000

df_non_sarcasm = df_en_non_sarcasm.sample(frac=1).iloc[0:non_sarcasm_num, :] 
df_sarcasm = df_en_sarcasm.sample(frac=1).iloc[0:sarcasm_num, :] 
df_jokes = df_ru_jokes.sample(frac=1).iloc[0:jokes_num, :] 

df = pd.concat([df_non_sarcasm, df_sarcasm, df_jokes]).sample(frac=1).reset_index(drop=True)

# delete punctuation
df = df.replace(to_replace=r'\r', value='', regex=True)
df = df.replace(to_replace=r'\n', value='', regex=True)
df.comment = df.comment.str.replace('[^\w\s]','')

sns.set_style("dark")
sns.countplot(df.label)

df.head()

## proportion train-test
n_train = 0.7
n_test = 0.3

ind = round(len(df)*n_train)
df_train = df.iloc[0:ind, :] 
# df_test = df.iloc[ind:, :] 
print(len(df_train))
# print(len(df_test))

# df_train.to_csv('/content/drive/MyDrive/Sarcastic_сomments/train_final_no_jokes.csv', index=False)

df_train.to_csv('/content/drive/MyDrive/Sarcastic_сomments/train_final.csv', index=False)
df_test.to_csv('/content/drive/MyDrive/Sarcastic_сomments/test_final.csv', index=False)

"""## LogReg (baseline score)

### Data preprocessing

Remove links (just in case)
"""

def remove_links(text): 
    return re.sub(r'http\S+', '', text)

"""Remove the text in brackets"""

def remove_between_brackets(text):
    return re.sub('\[[^]]*\]', '', text)

"""Preprosess data"""

def preprosess_data(dataframe):
    dataframe['comment'] = dataframe['comment'].apply(remove_links)
    dataframe['comment'] = dataframe['comment'].apply(remove_between_brackets)
    return dataframe

# df_train = pd.read_csv('/content/drive/MyDrive/Sarcastic_сomments/train_final.csv')
df_train = pd.read_csv('/content/drive/MyDrive/Sarcastic_сomments/train_final_no_jokes.csv')

df_test = pd.read_csv('/content/drive/MyDrive/Sarcastic_сomments/test_final.csv')

df_train_preprosessed = preprosess_data(df_train)
df_test_preprosessed = preprosess_data(df_test)

df_train_preprosessed.head()

"""Import data"""

X_train = df_train_preprosessed.comment
Y_train = df_train_preprosessed.label
X_test = df_test_preprosessed.comment
Y_test = df_test_preprosessed.label

"""Make a set of russian stop words including punctuation signs and digits"""

nltk.download("stopwords")
ru_stopwords = set(stopwords.words('russian'))
punctuation = list(string.punctuation) + [str(i) for i in range(10)] + ['']
ru_stopwords.update(punctuation)

print(ru_stopwords)

"""### Tf-Idf vectorizer"""

counter_tfidf = TfidfVectorizer(stop_words=ru_stopwords, ngram_range=(1,1), lowercase=True)

count_train = counter_tfidf.fit_transform(X_train)
count_test = counter_tfidf.transform(X_test)

print(count_train[0]) # the tf-idf is between [0,1]

"""### LogReg model """

model_lr = LogisticRegression(random_state = 12345, max_iter = 10000, n_jobs = -1) # -1 means using all processors (CPU cores)

model_lr.fit(count_train, Y_train)

eli5.show_weights(estimator=model_lr, feature_names=counter_tfidf.get_feature_names(), top=(10, 10))

"""Predictions"""

def test_data_analysis(Y_pred, Y_test):
    precision = accuracy_score(Y_test, Y_pred)
    recall = recall_score(Y_test, Y_pred, average='micro')
    F1_score = f1_score(Y_test, Y_pred, average='micro')
    print("Precision: ", precision)
    print("Recall: ", recall)
    print("F1_score: ", F1_score) 
    data = {'y_Actual': Y_test, 'y_Predicted': Y_pred}
    df = pd.DataFrame(data, columns=['y_Actual','y_Predicted'])
    confusion_matrix = pd.crosstab(df['y_Actual'], df['y_Predicted'], rownames=['Actual'], colnames=['Predicted'])
    plt.figure(figsize = (8, 6))
    sn.heatmap(confusion_matrix, annot=True)

Y_pred = model_lr.predict(count_test)
test_data_analysis(Y_pred, Y_test)

"""## Transformers (good score)

Links/Tutorials:


*   https://habr.com/ru/post/562064/
*   https://huggingface.co/cointegrated/rubert-tiny

### Customized Dataset class

The __len__ method returns the length of our dataset. The __getitem__ method returns a dictionary that consists of the source text itself, a list of tokens, an attention mask, and a class label. In __encoding__, we indicate to the tokenizer that the source text should be framed with service tokens add_special_tokens=True, and also supplement the received vectors to the maximum length padding='max_len'.
"""

class CustomDataset(Dataset):

  def __init__(self, texts, targets, tokenizer, max_len=512):
    self.texts = texts
    self.targets = targets
    self.tokenizer = tokenizer
    self.max_len = max_len

  def __len__(self):
    return len(self.texts)

  def __getitem__(self, idx):
    text = str(self.texts[idx][:self.max_len])
    target = self.targets[idx]

    encoding = self.tokenizer.encode_plus(
        text,
        add_special_tokens=True,
        max_length=self.max_len,
        return_token_type_ids=False,
        padding='max_length',
        return_attention_mask=True,
        return_tensors='pt',
    )

    return {
      'text': text,
      'input_ids': encoding['input_ids'].flatten(),
      'attention_mask': encoding['attention_mask'].flatten(),
      'targets': torch.tensor(target, dtype=torch.long)
    }

"""### BERT Classifier model

Downloading the Russian-language BERT model from the huggingface rupert-tiny repository. https://huggingface.co/cointegrated/rubert-tiny

For classification, it is necessary to add a fully connected layer, the number of inputs of which is the internal dimension of the embedding of the network, and the output is the number of classes for classification. In our case, we classify into 2 classes, and the internal dimension can be obtained by running the following command:
"""

class BertClassifier:

    def __init__(
        self,
        model_path,
        tokenizer_path,
        n_classes=2,
        epochs=1,
        batch_size=2,
        model_save_path='/content/bert.pt'
        ):
        self.model = BertForSequenceClassification.from_pretrained(model_path)
        self.tokenizer = BertTokenizer.from_pretrained(tokenizer_path)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model_save_path=model_save_path
        self.max_len = 512
        self.batch_size = batch_size
        self.epochs = epochs
        self.out_features = self.model.bert.encoder.layer[1].output.dense.out_features
        self.model.classifier = torch.nn.Linear(self.out_features, n_classes)
        self.model.to(self.device)
    
    def preparation(self, X_train, y_train, X_valid, y_valid):
        # create datasets
        self.train_set = CustomDataset(X_train, y_train, self.tokenizer, self.max_len)
        self.valid_set = CustomDataset(X_valid, y_valid, self.tokenizer, self.max_len)

        # create data loaders
        self.train_loader = DataLoader(self.train_set, self.batch_size, shuffle=True)
        self.valid_loader = DataLoader(self.valid_set, self.batch_size, shuffle=True)

        # helpers initialization
        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=2e-5)
        self.scheduler = get_linear_schedule_with_warmup(
                self.optimizer,
                num_warmup_steps=0,
                num_training_steps=len(self.train_loader) * self.epochs
                )
        self.loss_fn = torch.nn.CrossEntropyLoss().to(self.device)
            
    def fit(self):
        self.model.train()
        losses = []
        correct_predictions = 0
        loader = tqdm(self.train_loader)
        for data in loader:
            input_ids = data["input_ids"].to(self.device)
            attention_mask = data["attention_mask"].to(self.device)
            targets = data["targets"].to(self.device)

            outputs = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask
                )

            preds = torch.argmax(outputs.logits, dim=1)
            loss = self.loss_fn(outputs.logits, targets)

            correct_predictions += torch.sum(preds == targets)

            losses.append(loss.item())
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()
            self.scheduler.step()
            self.optimizer.zero_grad()

        train_acc = correct_predictions.double() / len(self.train_set)
        train_loss = np.mean(losses)
        return train_acc, train_loss
    
    def valid(self):
        self.model.eval()
        losses = []
        correct_predictions = 0

        with torch.no_grad():
            loader = tqdm(self.valid_loader)
            for data in loader:
                input_ids = data["input_ids"].to(self.device)
                attention_mask = data["attention_mask"].to(self.device)
                targets = data["targets"].to(self.device)

                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask
                    )

                preds = torch.argmax(outputs.logits, dim=1)
                loss = self.loss_fn(outputs.logits, targets)
                correct_predictions += torch.sum(preds == targets)
                losses.append(loss.item())
        
        val_acc = correct_predictions.double() / len(self.valid_set)
        val_loss = np.mean(losses)
        return val_acc, val_loss
    
    def train(self):
        best_accuracy = 0
        for epoch in range(self.epochs):
            print(f'Epoch {epoch + 1}/{self.epochs}')
            train_acc, train_loss = self.fit()
            print(f'Train loss {train_loss} accuracy {train_acc}')

            val_acc, val_loss = self.valid()
            print(f'Val loss {val_loss} accuracy {val_acc}')
            print('-' * 10)

            if val_acc > best_accuracy:
                torch.save(self.model, self.model_save_path)
                best_accuracy = val_acc

        # self.model = torch.load(self.model_save_path)
    
    def predict(self, text):
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            return_token_type_ids=False,
            truncation=True,
            padding='max_length',
            return_attention_mask=True,
            return_tensors='pt',
        )
        
        out = {
              'text': text,
              'input_ids': encoding['input_ids'].flatten(),
              'attention_mask': encoding['attention_mask'].flatten()
          }
        
        input_ids = out["input_ids"].to(self.device)
        attention_mask = out["attention_mask"].to(self.device)
        
        outputs = self.model(
            input_ids=input_ids.unsqueeze(0),
            attention_mask=attention_mask.unsqueeze(0)
        )
        
        prediction = torch.argmax(outputs.logits, dim=1).cpu().numpy()[0]

        return prediction

    def test_data_analysis(self, X_test, Y_test):
        pred = [self.predict(comment) for comment in X_test]
        precision = accuracy_score(Y_test, pred)
        recall = recall_score(Y_test, pred, average='micro')
        F1_score = f1_score(Y_test, pred, average='micro')
        print("Precision: ", precision)
        print("Recall: ", recall)
        print("F1_score: ", F1_score) 
        data = {'y_Actual': Y_test, 'y_Predicted': pred}
        df = pd.DataFrame(data, columns=['y_Actual','y_Predicted'])
        confusion_matrix = pd.crosstab(df['y_Actual'], df['y_Predicted'], rownames=['Actual'], colnames=['Predicted'])
        plt.figure(figsize = (8, 6))
        sn.heatmap(confusion_matrix, annot=True)

"""### Set, train, test

Set params
"""

from transformers import BertTokenizer, BertForSequenceClassification
classifier = BertClassifier(
        model_path='cointegrated/rubert-tiny2',
        tokenizer_path='cointegrated/rubert-tiny2',
        n_classes=2,
        epochs=10,
        batch_size=64,
        model_save_path='/content/drive/MyDrive/Sarcastic_сomments/bert.pt'
)

"""Make preparation"""

df_train = pd.read_csv('/content/drive/MyDrive/Sarcastic_сomments/train_final.csv')
# df_train = pd.read_csv('/content/drive/MyDrive/Sarcastic_сomments/train_final_no_jokes.csv')
df_test = pd.read_csv('/content/drive/MyDrive/Sarcastic_сomments/test_final.csv')

X = df_train.comment
Y = df_train.label

train_texts, val_texts, train_labels, val_labels = train_test_split(X, Y, test_size=0.15)

classifier.preparation(
        X_train=list(train_texts),
        y_train=list(train_labels),
        X_valid=list(val_texts),
        y_valid=list(val_labels)
    )

"""Train our model"""

classifier.train()

"""Predict zero comment"""

# normal sentence
print(classifier.predict("Позвони мне завтра вечером"))

# joke
print(classifier.predict("В дверь никто не постучал. «Пустое множество» — подумал Штирлиц"))

# sarcasm
print(classifier.predict("Продолжайте говорить. Я всегда зеваю, когда мне интересно"))

"""Test our model"""

X_test = df_test.comment
Y_test = df_test.label

classifier.test_data_analysis(X_test, Y_test)

"""## RNN (not so good score)"""

mvs_const = 30000

"""### Tokenization/Vocab/Embedding

Tokenization
"""

class Tokenizer:
    def __init__(self, word_pattern="[\w']+"):
        self.word_pattern = re.compile(word_pattern)

    def tokenize(self, text):
        return self.word_pattern.findall(text)

tok = Tokenizer()
def tokenize_func(text): 
    return [word for word in tok.tokenize(text)]

"""Preprocess the data"""

df_train = pd.read_csv('/content/drive/MyDrive/Sarcastic_сomments/train_final.csv')
# df_train = pd.read_csv('/content/drive/MyDrive/Sarcastic_сomments/train_final_no_jokes.csv')
df_train_preprosessed = df_train
df_train_preprosessed['comment'] = df_train_preprosessed['comment'].apply(tokenize_func)

"""Building vocab class (NLP HW2 code)"""

class Vocab:
    def __init__(self, tokenized_texts, max_vocab_size=None):
        """
        Builds a vocabulary by concatenating all tokenized texts and counting words.
        Most common words are placed in vocabulary, others are replaced with [UNK] token
        :param tokenized_texts: texts to build a vocab
        :param max_vocab_size: amount of words in vocabulary
        """
        counts = Counter(chain(*tokenized_texts))
        max_vocab_size = max_vocab_size or len(counts)
        common_pairs = counts.most_common(max_vocab_size)
        self.PAD_IDX = 0
        self.UNK_IDX = 1
        self.EOS_IDX = 2
        self.itos = [pair[0] for pair in common_pairs]
        self.stoi = {token: i for i, token in enumerate(self.itos)}

    def vectorize(self, text):
        """
        Maps each token to it's index in the vocabulary
        :param text: sequence of tokens
        :return: vectorized sequence
        """
        return [self.stoi.get(tok, self.UNK_IDX) for tok in text]

    def __iter__(self):
        return iter(self.itos)

    def __len__(self):
        return len(self.itos)

"""Embedding matrix create function (NLP HW2 code)"""

def prepare_emb_matrix(gensim_model, vocab: Vocab):

    mean = gensim_model.vectors.mean(1).mean()
    std = gensim_model.vectors.std(1).mean()
    vec_size = gensim_model.vector_size
    emb_matrix = torch.zeros((len(vocab), vec_size))
    for i, word in enumerate(vocab.itos[1:], 1):
        try:
            emb_matrix[i] = torch.tensor(gensim_model.get_vector(word))
        except KeyError:
            emb_matrix[i] = torch.randn(vec_size) * std + mean
    return emb_matrix

vocab = Vocab(df_train_preprosessed['comment'].to_list(), max_vocab_size=mvs_const)

"""https://github.com/natasha/navec -- not bad glove model's"""

!pip install navec
!wget https://storage.yandexcloud.net/natasha-navec/packs/navec_hudlit_v1_12B_500K_300d_100q.tar

from navec import Navec

path = 'navec_hudlit_v1_12B_500K_300d_100q.tar'
navec = Navec.load(path)
glove_model = navec.as_gensim

"""Embedding matrix creation (there are much bigger and better models, it's test)"""

emb_matrix = prepare_emb_matrix(glove_model, vocab)

"""### Class RecurrentClassifier"""

class RecurrentClassifier(torch.nn.Module):
    def __init__(self, vocab, emb_matrix):
        super().__init__()
        self.vocab = vocab
        self.emb_matrix = emb_matrix
        self.embeddings = torch.nn.Embedding.from_pretrained(emb_matrix, freeze=True,
                                                             padding_idx=vocab.PAD_IDX)

        self.cell = torch.nn.GRU(input_size=emb_matrix.size(1),
                                 batch_first=True,
                                 hidden_size=100,
                                 num_layers=3,
                                 dropout=0.1,
                                 bidirectional=True,
                                 )

        self.out_activation = torch.nn.Sigmoid()
        self.out_dropout = torch.nn.Dropout(0.1)
        cur_out_size = 600
        out_layers = []
        for cur_hidden_size in [40]:
            out_layers.append(torch.nn.Linear(cur_out_size, cur_hidden_size))
            cur_out_size = cur_hidden_size
        out_layers.append(torch.nn.Linear(cur_out_size, 2))
        # out_layers.append(self.out_dropout)
        out_layers.append(self.out_activation)
        self.out_proj = torch.nn.Sequential(*out_layers)

    def forward(self, input):
        embedded = self.embeddings(input.data)
        states, last_state = self.cell(torch.nn.utils.rnn.PackedSequence(embedded,
                                                                         input.batch_sizes,
                                                                         sorted_indices=input.sorted_indices,
                                                                         unsorted_indices=input.unsorted_indices))
        
        if isinstance(last_state, tuple):
            last_state = last_state[0]
        last_state = last_state.transpose(0, 1)
        last_state = last_state.reshape(last_state.size(0), -1)
        return self.out_proj(last_state)

"""### Class Trainer"""

class Trainer:
    def __init__(self, config):

        self.config = config
        self.n_epochs = config["n_epochs"]
        self.setup_opt_fn = lambda model: torch.optim.Adam(model.parameters(),
                                                           config["lr"],
                                                           weight_decay=config["weight_decay"])
        # self.setup_sheduler = lambda model: torch.optim.lr_scheduler.StepLR(self.setup_opt_fn(model),
        #                                                                     step_size=14, gamma=0.4)
        self.model = None
        self.opt = None
        self.sheduler = None
        self.history = None
        self.loss_fn = torch.nn.CrossEntropyLoss()
        self.device = config["device"]
        self.verbose = config.get("verbose", True)

    def fit(self, model, train_loader, val_loader):

        self.model = model.to(self.device)
        self.opt = self.setup_opt_fn(self.model)
        # self.sheduler = self.setup_sheduler(self.model)
        self.history = {"train_loss": [], "val_loss": [], "val_acc": []}
        for epoch in range(self.n_epochs):
            print(f"epoch: {epoch+1}")
            train_info = self._train_epoch(train_loader)
            val_info = self._val_epoch(val_loader)
            self.history["train_loss"].extend(train_info["train_loss"])
            self.history["val_loss"].append(val_info["loss"])
            self.history["val_acc"].append(val_info["acc"])
            # self.save(f"/content/drive/MyDrive/NLP_HW2_data/gensim_model_{epoch+1}.ckpt")
            # if epoch != 0:
            #    self.sheduler.step()
        return self.model.eval()

    def _train_epoch(self, train_loader):
        self.model.train()
        losses = []
        if self.verbose:
            train_loader = tqdm(train_loader)
        for batch in train_loader:
            self.model.zero_grad()
            texts, labels = batch
            logits = self.model.forward(texts.to(self.device))
            loss = self.loss_fn(logits, labels.to(self.device))
            loss.backward()
            self.opt.step()
            loss_val = loss.item()
            losses.append(loss_val)
        if self.verbose:
            train_loader.set_description(f"Loss={np.mean(losses):.3}")
            print(f"Loss={np.mean(losses):.3}")
        return {"train_loss": losses}

    def _val_epoch(self, val_loader):
        self.model.eval()
        all_logits = []
        all_labels = []
        if self.verbose:
            val_loader = tqdm(val_loader)
        with torch.no_grad():
            for batch in val_loader:
                texts, labels = batch
                logits = self.model.forward(texts.to(self.device))
                all_logits.append(logits)
                all_labels.append(labels)
        all_labels = torch.cat(all_labels).to(self.device)
        all_logits = torch.cat(all_logits)
        loss = self.loss_fn(all_logits, all_labels).item()
        acc = (all_logits.argmax(1) == all_labels).float().mean().item()
        if self.verbose:
            val_loader.set_description(f"Loss={loss:.3}; Acc:{acc:.3}")
            print(f"Loss={loss:.3}; Acc:{acc:.3}")
        return {"acc": acc, "loss": loss}

    def predict(self, test_loader):
        if self.model is None:
            raise RuntimeError("You should train the model first")
        self.model.eval()
        predictions = []
        with torch.no_grad():
            for batch in test_loader:
                texts, labels = batch
                logits = self.model.forward(texts.to(self.device))
                predictions.extend(logits.argmax(1).tolist())
        return np.asarray(predictions)

"""### Class TextDataset

pandas to dataset (save time)
"""

class TextDataset(torch.utils.data.Dataset):
    def __init__(self, tokenized_texts, labels, vocab: Vocab):
        self.texts = tokenized_texts
        self.labels = labels
        self.vocab = vocab

    def __getitem__(self, item):
        return self.vocab.vectorize(self.texts[item]) + [self.vocab.EOS_IDX], self.labels[item]

    def __len__(self):
        return len(self.texts)

    def collate_fn(self, batch):
        """
        Technical method to form a batch to feed into recurrent network
        """
        return torch.nn.utils.rnn.pack_sequence([torch.tensor(pair[0]) for pair in batch], 
                                                enforce_sorted=False), torch.tensor([pair[1] for pair in batch])

"""### Set, train, test

Creating dataloaders
"""

X = df_train_preprosessed.comment
Y = df_train_preprosessed.label

train_texts, val_texts, train_labels, val_labels = train_test_split(X, Y, test_size=0.2)
train_dataset = TextDataset(train_texts.to_list(), train_labels.to_list(), vocab)
val_dataset = TextDataset(val_texts.to_list(), val_labels.to_list(), vocab)

train_dataloader = torch.utils.data.DataLoader(train_dataset, 
                                               batch_size=64,
                                               shuffle=True,
                                               collate_fn=train_dataset.collate_fn)
val_dataloader = torch.utils.data.DataLoader(val_dataset, 
                                             batch_size=64,
                                             shuffle=False,
                                             collate_fn=val_dataset.collate_fn)

"""Params"""

trainer_config = {
    "lr": 1e-3,
    "n_epochs": 10,
    "weight_decay": 1e-6,
    "device": "cuda" if torch.cuda.is_available() else "cpu"
}

# torch.seed()
clf_model = RecurrentClassifier(vocab, emb_matrix)

t = Trainer(trainer_config)
t.fit(clf_model, train_dataloader, val_dataloader)

def predict(model, comment):
    tok_text = tok.tokenize(comment)
    indexed_text = torch.tensor(vocab.vectorize(tok_text)).to(t.device)
    label = model(torch.nn.utils.rnn.pack_sequence([indexed_text])).argmax().item()
    return label

print(predict(t.model, "Завтра пойду в магазин, ты со мной?"))
print(predict(t.model, "Мне смешно"))
print(predict(t.model, "Продолжайте говорить. Я всегда зеваю, когда мне интересно"))

def test_data_analysis(X_test, Y_test):
    Y_pred = [predict(t.model, comment) for comment in X_test]
    precision = accuracy_score(Y_test, Y_pred)
    recall = recall_score(Y_test, Y_pred, average='micro')
    F1_score = f1_score(Y_test, Y_pred, average='micro')
    print("Precision: ", precision)
    print("Recall: ", recall)
    print("F1_score: ", F1_score) 
    data = {'y_Actual': Y_test, 'y_Predicted': Y_pred}
    df = pd.DataFrame(data, columns=['y_Actual','y_Predicted'])
    confusion_matrix = pd.crosstab(df['y_Actual'], df['y_Predicted'], rownames=['Actual'], colnames=['Predicted'])
    plt.figure(figsize = (8, 6))
    sn.heatmap(confusion_matrix, annot=True)

df_test = pd.read_csv('/content/drive/MyDrive/Sarcastic_сomments/test_final.csv')
X_test = df_test.comment
Y_test = df_test.label

test_data_analysis(X_test, Y_test)

